{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# PCA Warmup\n", "hell yeah\n", "\n", "## Concepts\n", "\n", "#### Give at least two reasons why one might use PCA.  Be sure to include discussion of what problem PCA solves and how."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your answers here"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# Run as-is\n", "from sklearn.datasets import make_classification\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.decomposition import PCA\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import f1_score\n", "from sklearn.model_selection import cross_validate\n", "from sklearn.preprocessing import StandardScaler\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "%matplotlib inline\n", "import pickle as pkl\n", "\n", "with open('test_obj/X.pkl', 'rb') as f:\n", "    X = pkl.load(f)\n", "\n", "with open('test_obj/y.pkl', 'rb') as f:\n", "    y = pkl.load(f)  \n", "\n", "from test_scripts.test_class import Test\n", "test = Test()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Application\n", "\n", "### PCA Exploration\n", "\n", "#### Run the following steps\n", "- TTS, `random_state` = 1\n", "\n", "- Scale w/ StandardScaler\n", "\n", "- For the full PCA transformation (ie, w/o specifying `n_components` number)\n", "on `X_train`, store the explained variance ratio for each component \n", "in `evr`\n", "\n", "- Graph `evr`\n", "\n", "- Re-graph the first few components in order to focus on them (you'll see why)\n", "\n", "How many principal components does it look like we should choose?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your work here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Calculate Train and Test Error\n", "\n", "#### Run the following:\n", "- for the first 50 principal components,\n", "  - scale the data\n", "  - generate the number of principal components and transform them into features\n", "  - generate train and test predictions through 10-fold cross validation\n", "    - *Hint:* use `sklearn.cross_validate`\n", "    - *Level up:* ideally, you would scale and PCA on each training fold (why?).  Use a pipeline!\n", "  - graph the means of the train and test predictions for each number of principal component\n", "\n", "#### Interpret the graph  \n", "- How many principle components should we choose based on this?\n", "- Is overfitting or underfitting a problem?\n", "\n", "#### Level up\n", "- Repeat the above but don't scale the data.  Intepret the results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your work here"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}